<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Zongheng Tang</title>
    <meta name="author" content="Zongheng Tang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zongheng Tang (汤宗衡)
                </p>
                <p>
                  I'm a postdoctoral fellow at the Hangzhou International Innovation Institute, Beihang University, fortunate to be co-advised by Prof. Zhiming Zheng and Prof. Si Liu. I received my Ph.D. and B.Eng. degrees in Computer Science from Beihang University.
                </p>
                <p>
                  My research interests focus on <strong>computer vision</strong> and <strong>embodied AI</strong>. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:tzhhhh123@buaa.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/Zongheng-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=jrgMNxEAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/tzhhhh123">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/tzh.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/tzh.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  Some selected papers are <span class="highlight">highlighted</span>. (* denotes equal contribution)
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px 15px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr bgcolor="#ffffd0">
            <td style="padding:16px;width:35%;vertical-align:middle">
              <img src='images/cost.jpg' style="width:100%; max-width:100%; border-radius: 4px;">
            </td>
            <td style="padding:8px;width:65%;vertical-align:middle">
              <a href="#">
                <span class="papertitle">CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective</span>
              </a>
              <br>
              <strong>Zongheng Tang</strong>, Yi Liu, Yifan Sun, Yulu Gao, Jinyu Chen, Runsheng Xu, Si Liu
              <br>
              <em>ICCV</em>, 2025 &nbsp; <font color="red"><strong>(Highlight)</strong></font>
              <br>
              <a href="#">project page</a> / <a href="#">arXiv</a>
              <p></p>
              <p>Proposed a unified spatiotemporal perspective for collaborative perception, significantly reducing communication bandwidth while improving 3D object detection accuracy.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:35%;vertical-align:middle">
              <img src='images/detr_ga.jpg' style="width:100%; max-width:100%; border-radius: 4px;">
            </td>
            <td style="padding:8px;width:65%;vertical-align:middle">
              <a href="#">
                <span class="papertitle">Detr with additional global aggregation for cross-domain weakly supervised object detection</span>
              </a>
              <br>
              <strong>Zongheng Tang</strong>, Yifan Sun, Si Liu, Yi Yang
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="#">project page</a> / <a href="#">arXiv</a>
              <p></p>
              <p>Introduced a domain adaptive paradigm based on the DETR framework with a novel encoder-decoder dual-level global Query aggregation module.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:35%;vertical-align:middle">
              <img src='images/mi3c.jpg' style="width:100%; max-width:100%; border-radius: 4px;">
            </td>
            <td style="padding:8px;width:65%;vertical-align:middle">
              <a href="#">
                <span class="papertitle">MI3C: Mining intra-and inter-image context for person search</span>
              </a>
              <br>
              <strong>Zongheng Tang*</strong>, Yulu Gao*, Tianrui Hui, Fengguang Peng, Si Liu
              <br>
              <em>Pattern Recognition (PR)</em>, 2024
              <br>
              <a href="#">project page</a> / <a href="#">arXiv</a>
              <p></p>
              <p>A novel approach for person search that effectively mines contextual information both within and across images.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:35%;vertical-align:middle">
              <img src='images/hcvg.jpg' style="width:100%; max-width:100%; border-radius: 4px;">
            </td>
            <td style="padding:8px;width:65%;vertical-align:middle">
              <a href="#">
                <span class="papertitle">Human-centric spatio-temporal video grounding with visual transformers</span>
              </a>
              <br>
              <strong>Zongheng Tang</strong>, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, Dong Xu
              <br>
              <em>TCSVT</em>, 2021
              <br>
              <a href="#">project page</a> / <a href="#">arXiv</a>
              <p></p>
              <p>Proposed the human-centric spatio-temporal video grounding task and established a popular benchmark dataset and baseline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:35%;vertical-align:middle">
              <img src='images/tcsvt_tracking.jpg' style="width:100%; max-width:100%; border-radius: 4px;">
            </td>
            <td style="padding:8px;width:65%;vertical-align:middle">
              <a href="#">
                <span class="papertitle">Unified transformer with isomorphic branches for natural language tracking</span>
              </a>
              <br>
              Rong Wang*, <strong>Zongheng Tang*</strong>, Qianli Zhou, Xiaoqian Liu, Tianrui Hui, Quange Tan, Si Liu
              <br>
              <em>TCSVT</em>, 2023
              <br>
              <a href="#">project page</a> / <a href="#">arXiv</a>
              <p></p>
              <p>Designed a language-vision isomorphic dual-branch architecture for highly accurate language-guided object tracking.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:35%;vertical-align:middle">
              <img src='images/tracker.jpg' style="width:100%; max-width:100%; border-radius: 4px;">
            </td>
            <td style="padding:8px;width:65%;vertical-align:middle">
              <a href="#">
                <span class="papertitle">Strong Detector with Simple Tracker</span>
              </a>
              <br>
              <strong>Zongheng Tang*</strong>, Yulu Gao*, Zizheng Xun*, Fengguang Peng*, Yifan Sun, Si Liu, Bo Li
              <br>
              <em>CVPRW</em>, 2023
              <br>
              <a href="#">project page</a> / <a href="#">arXiv</a>
              <p></p>
              <p>Developed a joint UAV detection and tracking model integrating trajectory prediction and appearance feature matching.</p>
            </td>
          </tr>

          </tbody></table>

          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Competitions & Awards</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:8px;width:100%;vertical-align:middle">
                <ul>
                  <li>
                    <strong>Champion</strong> (1st place) - CVPR 2023 3rd Anti-UAV Tracking Challenge <em>(Lead Participant)</em>
                  </li>
                  <li>
                    <strong>3rd Place</strong> - CVPR 2023 3rd Anti-UAV Detection Challenge <em>(Lead Participant)</em>
                  </li>
                  <li>
                    <strong>Runner-up</strong> (2nd place) - ICCV 2022 2nd Anti-UAV Tracking Challenge <em>(Core Participant)</em>
                  </li>
                  <li>
                    <strong>Champion</strong> (1st place) - ACL 2020 REVERIE Vision-Language Navigation Challenge <em>(Core Participant)</em>
                  </li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>